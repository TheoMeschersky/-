{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmgQw_NQYBCg"
      },
      "source": [
        "# Python и интернет. Модуль requests\n",
        "\n",
        "**План**:\n",
        "\n",
        "1. Requests\n",
        "4. Beautiful Soup\n",
        "5. Краулеры"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Как устроен интернет\n",
        "\n",
        "Оригинальные [конспект1](https://github.com/elmiram/2016learnpython/blob/master/7%20%D0%A1%D0%B5%D0%BC%D0%B8%D0%BD%D0%B0%D1%80%20-%20flask%20intro.ipynb) [конспект2](https://github.com/elmiram/2016learnpython/blob/master/8%20%D0%A1%D0%B5%D0%BC%D0%B8%D0%BD%D0%B0%D1%80%20-%20%D0%A1%D0%BD%D0%BE%D0%B2%D0%B0%20flask.ipynb)\n",
        "\n",
        "__Веб-сервер__\n",
        "* устройство vs. программа\n",
        "* задача — получать запросы от других компьютеров или программ (клиентов, clients) и отправлять запрошенные данные\n",
        "* основная функция — размещение сайтов (website hosting)\n",
        "\n",
        "__Термины__\n",
        "* *Запрос (request)* — сообщение от агента, желающего получить или разместить информацию\n",
        "* *Ответ (response)* — ответное сообщение сервера с кодом состояния и информацией (HTML, изображения, загружаемые файлы и т. п.)\n",
        "* *Протокол (protocol)* — набор правил, по которым составляются запросы и ответы\n",
        "* *Сессия (session)* — установка соединения между агентом и сервером и последующая серия запросов и ответов\n",
        "* *Гипертекст* — множество текстов, организованных в виде графа при помощи гиперссылок\n",
        "* *Протокол HTTP (HyperText Transfer Protocol)* — протокол для передачи гипертекстовых данных (обычно в виде HTML)\n",
        "* *URL (Uniform Resource Locator)*, веб-адрес ресурса — строка, представляющая собой уникальное имя, по которому можно найти в сети этот ресурс\n",
        "\n",
        "__Адресация__\n",
        "\n",
        "IP\n",
        "* *IP-адрес* — (пока что) набор из 4 байт, присваиваемый каждому подключённому к сети устройству\n",
        "* Некоторые IP-адреса уникальны, некоторые — нет (внутренние адреса в локальных сетях)\n",
        "* Практически любой ресурс (например, сайт) можно получить по его IP-адресу (например, через браузер)\n",
        "* Существуют зарезервированные адреса и диапазоны адресов, например, `127.0.0.1` — адрес данного устройства\n",
        "\n",
        "Порт\n",
        "* Каждый запрос обращается не просто к какому-то IP-адресу, а к некоторому порту на этом адресе\n",
        "* Веб-сервер имеет 65 535 портов, пронумерованных начиная с 1\n",
        "* Веб-сервер может прослушивать некоторые порты (listen to ports) и по-разному обрабатывать сообщения, поступившие на разные порты\n",
        "* Если порт не прослушивается, сообщения на этот порт останутся без ответа\n",
        "\n",
        "\n",
        "__URL__\n",
        "\n",
        "`http__://__www.example.com__:__1234__/__directory1/file1.html`\n",
        "\n",
        "`протокол__://__доменное имя или IP-адрес__:__порт__/__адрес файла на сервере`\n",
        "\n",
        "* Порт указывать не обязательно: используются стандартные порты (HTTP — 80, FTP — 20 и т. п.)\n",
        "* *DNS (Domain Name Servers)* — специальные сервера в сети, на которых хранятся таблицы соответствия между доменными именами и IP-адресами их серверов\n",
        "\n",
        "\n",
        "__HTML и скрипты__\n",
        "* страницы, содержимое (HTML-код) которых не изменяется, называются *статическими (static)*\n",
        "* страницы, содержимое которых может быть разным в зависимости от введённых пользователем данных, времени, IP-адреса и т. п., называются *динамическими (dynamic)*\n",
        "* динамические страницы создаются с помощью скриптов *на стороне сервера (server side scripts)*, написанных, например, на PHP или Python\n",
        "* скрипт порождает HTML и посылает его пользователю\n",
        "* пользователь не видит кода скрипта, выполняющегося на сервере\n",
        "* *скрипт на стороне клиента (client side script)* — вставка в HTML на каком-то языке программирования (например, JavaScript), позволяющая странице вести себя интерактивно\n",
        "* код клиентских скриптов посылается клиенту сервером вместе с HTML и не выполняется на сервере\n",
        "\n",
        "__Питон и сайты__\n",
        "\n",
        "Написать сайт на питоне значит написать такую программу, которая может работать веб-сервером или использоваться веб-сервером для порождения HTML-кода веб-страниц. Для этого существует несколько модулей, например, Django и Flask. Мы их рассмотрим на следующих парах.\n",
        "\n",
        "А пока выясним, как данные из интернета доставать."
      ],
      "metadata": {
        "id": "EK5Xt0t6PILn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMdJJbokYBCi"
      },
      "source": [
        "## Как выкачать интернет\n",
        "Современный Интернет предоставляет большое количество языковых данных: электронные газеты и журналы, блоги, форумы, социальные сети и т.д. Например, можно найти в сети много-много текстов и собрать корпус, или найти все газетные статьи и блог-посты про какую-нибудь корпорацию и проанализировать тональность сообщений. Сейчас мы научимся заниматься выкачиванием страниц из интернета с помощью Python.\n",
        "\n",
        "Для скачивания HTML-страниц в питоне есть несколько библиотек: **requests** и **urllib**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLNYPfbMYBCj"
      },
      "source": [
        "## Requests\n",
        "\n",
        "Можно почитать в [документации](https://requests.readthedocs.io/en/latest/) или [тут](https://realpython.com/python-requests/).\n",
        "\n",
        "Допустим, мы хотим скачать главную страницу Хабра.\n",
        "\n",
        "На самом деле, когда мы хотим открыть какую-то страницу в интернете, наш браузер отправляет на сервер **запрос** (\"Привет, сервер! я хочу код страницы по вот такому адресу!\"), а сервер затем отправляет ответ (\"Привет! Вот код страницы: ...\").\n",
        "Чтобы получить страницу через питон, нужно сформировать **запрос** на сервер так же, как это делает браузер:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "txfW0gWgYBCk"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eR86pOlQYBCl"
      },
      "outputs": [],
      "source": [
        "response = requests.get(\"https://habr.com/ru/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5T6bjMBYBCl"
      },
      "source": [
        "В response теперь лежит ответ сервера. Это не просто HTML-код страницы, а еще дополнительная информация. Если мы просто выведем этот `response`, нам покажется только код (если статус $-$ 200, то все ок; вам, вероятно, хорошо известен статус 404)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g53d-WU5YBCl",
        "outputId": "87f56827-660b-4ec8-97d3-a2d3659ecc5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_vVB0dsYBCm",
        "outputId": "3243cf50-d8d9-471e-ca23-370e0cac1793"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "response.status_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XirrpWlIYBCm"
      },
      "source": [
        "А вот в атрибуте `text` уже лежит HTML-код"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQshGk0KYBCm",
        "outputId": "dff8957e-1900-41ad-ef06-5feff05ae0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html lang=\"ru\">\n",
            "\n",
            "  <head>\n",
            "    <title>Публикации &#x2F; Моя лента &#x2F; Хабр</title>\n",
            "<link rel=\"image_src\" href=\"/img/habr_ru.png\" data-hid=\"2a79c45\">\n",
            "<link href=\"https://habr.com/ru/feed/\" rel=\"canonical\" data-hid=\"e3fa780\">\n",
            "<meta itemprop=\"image\" content=\"/img/habr_ru.png\">\n",
            "<meta property=\"og:image\" content=\"/img/habr_ru.png\">\n",
            "<meta property=\"og:image:width\" content=\"1200\">\n",
            "<meta property=\"og:image:height\" content=\"630\">\n",
            "<meta property=\"aiturec:image\" content=\"/img/habr_ru.png\">\n",
            "<meta name=\"twitter:image\" content=\"/img/habr_ru.png\">\n",
            "<meta property=\"vk:image\" content=\"/img/habr_ru.png?format=vk\">\n",
            "<meta property=\"fb:app_id\" content=\"444736788986613\">\n",
            "<meta property=\"fb:pages\n"
          ]
        }
      ],
      "source": [
        "print(response.text[:700])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOFbk0oZYBCn"
      },
      "source": [
        "Можно посмотреть, что еще можно достать из response.\n",
        "\n",
        "Функция ```dir``` выдает список методов и параметров объекта."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fKF0vhmYBCn",
        "outputId": "59c3bfda-5568-4b1b-b9b6-85cd4d01a592"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apparent_encoding',\n",
              " 'close',\n",
              " 'connection',\n",
              " 'content',\n",
              " 'cookies',\n",
              " 'elapsed',\n",
              " 'encoding',\n",
              " 'headers',\n",
              " 'history',\n",
              " 'is_permanent_redirect',\n",
              " 'is_redirect',\n",
              " 'iter_content',\n",
              " 'iter_lines',\n",
              " 'json',\n",
              " 'links',\n",
              " 'next',\n",
              " 'ok',\n",
              " 'raise_for_status',\n",
              " 'raw',\n",
              " 'reason',\n",
              " 'request',\n",
              " 'status_code',\n",
              " 'text',\n",
              " 'url']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "[i for i in dir(response) if not i.startswith(\"_\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKCyxiHvYBCo"
      },
      "source": [
        "- Кодировка:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gkqXDUDHYBCo",
        "outputId": "c848595e-b296-4505-a640-73afea2fbbc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'utf-8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "response.encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy8svnQ7YBCo"
      },
      "source": [
        "- Заголовки (техническая информация):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GOa4MZsYBCo",
        "outputId": "f78c6b2d-7ba7-4d5f-80ad-b82cbc0d5f8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Server': 'QRATOR',\n",
              " 'Date': 'Sun, 16 Nov 2025 16:36:01 GMT',\n",
              " 'Content-Type': 'text/html; charset=utf-8',\n",
              " 'Transfer-Encoding': 'chunked',\n",
              " 'Connection': 'keep-alive',\n",
              " 'Keep-Alive': 'timeout=15',\n",
              " 'Vary': 'Accept-Encoding, Accept-Encoding',\n",
              " 'X-DNS-Prefetch-Control': 'off',\n",
              " 'X-Frame-Options': 'SAMEORIGIN',\n",
              " 'X-Download-Options': 'noopen',\n",
              " 'X-Content-Type-Options': 'nosniff',\n",
              " 'X-XSS-Protection': '1; mode=block',\n",
              " 'ETag': 'W/\"630b5-zrcW1fjmgJXcsvdNfev9zZRRkVU\"',\n",
              " 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains',\n",
              " 'X-Request-Id': '378d1423fa9ce134d91d1f9a1bbce449',\n",
              " 'X-Request-Geoip-Country-Code': 'US',\n",
              " 'X-Request-Detected-Device': 'mobile',\n",
              " 'Content-Encoding': 'gzip'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "dict(response.headers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fILLEeuYBCo"
      },
      "source": [
        "- Адрес запроса:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Rv6vs1D_YBCo",
        "outputId": "b7e966d1-7fb2-491e-d5a7-1889c4c0d46a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://habr.com/ru/feed/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "response.url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNxoq1hX9T3L"
      },
      "source": [
        "* Содержимое страницы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj9AzfewYBCp",
        "outputId": "83ba47b5-a163-439b-d881-4c1aecd4ff10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html lang=\"ru\">\n",
            "\n",
            "  <head>\n",
            "    <title>Публикации &#x2F; Моя лента &#x2F; Хабр</title>\n",
            "<link rel=\"image_src\" href=\"/img/habr_ru.png\" data-hid=\"2a79c45\">\n",
            "<link href=\"https://habr.com/ru/feed/\" rel=\"canonical\" data-hid=\"e3fa780\">\n",
            "<meta itemprop=\"image\" content=\"/img/habr_ru.png\">\n",
            "<meta \n"
          ]
        }
      ],
      "source": [
        "print(response.text[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wadZX78JYBCp"
      },
      "source": [
        "Ура, всё на месте!\n",
        "\n",
        "Но что всё это значит? Что такое HTML и как вообще из него доставать какую-то информацию?\n",
        "\n",
        "Ответ: по **тегам**! Например, в куске HTML сверху есть теги `<title> </title>` (теги всегда обрамляют с двух сторон то, что находится под этим тегом). В `<title>` в данном случае лежит заголовок этой интернет-страницы.\n",
        "\n",
        "Существует несколько вариантов, как достать что-то из определенного тега, например, достать заголовок:\n",
        "\n",
        "1. регулярные выражения ([плохой вариант](https://stackoverflow.com/questions/590747/using-regular-expressions-to-parse-html-why-not))\n",
        "2. специальные библиотеки питона, например, BeautifulSoup или [lxml](https://lxml.de/) (хороший вариант)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-AOP79BYBCp"
      },
      "source": [
        "## BeautifulSoup\n",
        "\n",
        "[Документация](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "\n",
        "Код страницы парсится как иерархия тегов (как они есть в html коде, один вложен в другой) и можно искать элементы с помощью разных методов, использовать атрибуты и т.д."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QVL8TDopYBCp"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RwP4AN9YBCp"
      },
      "source": [
        "Сначала инициализируем объект `BeautifulSoup`. Потом применим метод `find` и в скобочках укажем теги, по которым ищем. У некоторых тегов в HTML (как и в нашем случае) бывает еще и `class` и какие-нибудь еще атрибуты. Такие вещи мы задаем словариком.\n",
        "\n",
        "Этот запрос вернёт нам только первый заголовок. То есть первое вхождение такого тега в нашем HTML файле."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M28-oqzAYBCp",
        "outputId": "feae3d51-e45a-4856-a7fd-65195867a517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<h2 class=\"tm-title tm-title_h2\" data-test-id=\"articleTitle\">\n",
            " <!--[-->\n",
            " <a class=\"tm-title__link\" data-article-link=\"true\" data-test-id=\"article-snippet-title-link\" href=\"/ru/articles/966892/\">\n",
            "  <span>\n",
            "   Многопоточность без боли: моя шпаргалка для собесов в Java\n",
            "  </span>\n",
            " </a>\n",
            " <!--]-->\n",
            "</h2>\n",
            "\n",
            "Многопоточность без боли: моя шпаргалка для собесов в Java\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "soup = BeautifulSoup(response.text, 'html.parser')  # инициализируем (создаем) soup\n",
        "\n",
        "post = soup.find('h2', {'class': 'tm-title tm-title_h2'})\n",
        "print(post.prettify())\n",
        "print(post.get_text(), end=\"\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVN9z_TBYBCq"
      },
      "source": [
        "Но мы хотим получить все заголовки постов! Метод `find_all` возвращает массив всех элементов с тегом указанным в скобках. По нему можно итерироваться."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Itv0BRL6YBCq",
        "outputId": "f2185263-2d73-4473-b359-ca093d6609fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Многопоточность без боли: моя шпаргалка для собесов в Java\n",
            "<h2 class=\"tm-title tm-title_h2\" data-test-id=\"articleTitle\">\n",
            " <!--[-->\n",
            " <a class=\"tm-title__link\" data-article-link=\"true\" data-test-id=\"article-snippet-title-link\" href=\"/ru/articles/966892/\">\n",
            "  <span>\n",
            "   Многопоточность без боли: моя шпаргалка для собесов в Java\n",
            "  </span>\n",
            " </a>\n",
            " <!--]-->\n",
            "</h2>\n",
            "\n",
            "-- -- -- -- -- -- -- -- -- -- \n",
            "RAG+Ragas: учим AI-помощника учить без галлюцинаций\n",
            "<h2 class=\"tm-title tm-title_h2\" data-test-id=\"articleTitle\">\n",
            " <!--[-->\n",
            " <a class=\"tm-title__link\" data-article-link=\"true\" data-test-id=\"article-snippet-title-link\" href=\"/ru/companies/cloud_ru/articles/966698/\">\n",
            "  <span>\n",
            "   RAG+Ragas: учим AI-помощника учить без галлюцинаций\n",
            "  </span>\n",
            " </a>\n",
            " <!--]-->\n",
            "</h2>\n",
            "\n",
            "-- -- -- -- -- -- -- -- -- -- \n",
            "Архитектура фронтенда. Навеяно болью от использования FSD\n",
            "<h2 class=\"tm-title tm-title_h2\" data-test-id=\"articleTitle\">\n",
            " <!--[-->\n",
            " <a class=\"tm-title__link\" data-article-link=\"true\" data-test-id=\"article-snippet-title-link\" href=\"/ru/articles/966962/\">\n",
            "  <span>\n",
            "   Архитектура фронтенда. Навеяно болью от использования FSD\n",
            "  </span>\n",
            " </a>\n",
            " <!--]-->\n",
            "</h2>\n",
            "\n",
            "-- -- -- -- -- -- -- -- -- -- \n"
          ]
        }
      ],
      "source": [
        "for post in soup.find_all('h2', {'class': 'tm-title tm-title_h2'})[:3]:\n",
        "    print(post.get_text())\n",
        "    print(post.prettify())\n",
        "\n",
        "    print('-- ' * 10)  # для красоты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRbMZDOEYBCq"
      },
      "source": [
        "## Задание на семинар 1\n",
        "\n",
        "А что если мы хотим зайти еще глубже по дереву тегов и, например, для каждого заголовка поста найти никнейм юзера, который написал этот пост, и время написания поста?\n",
        "\n",
        "Для этого надо снова зайти в просмотор кода страницы и увидеть, что там проиcходит что-то такое:\n",
        "\n",
        "(Заодно обратите внимание, как пишутся комменты в HTML)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP1aFnk3Y1-x"
      },
      "source": [
        "```\n",
        "<article class=\"tm-articles-list__item\" data-navigatable=\"\" id=\"771476\" tabindex=\"0\">\n",
        " <div class=\"tm-article-snippet tm-article-snippet\">\n",
        "  <div class=\"tm-article-snippet__meta-container\">\n",
        "   <div class=\"tm-article-snippet__meta\">\n",
        "    <span class=\"tm-user-info tm-article-snippet__author\">\n",
        "     <a class=\"tm-user-info__userpic\" href=\"/ru/users/darinka666/\" title=\"darinka666\">\n",
        "      <div class=\"tm-entity-image\">\n",
        "       <img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" src=\"https://assets.habr.com/habr-web/img/avatars/093.png\" width=\"24\"/>\n",
        "      </div>\n",
        "     </a>\n",
        "     <span class=\"tm-user-info__user tm-user-info__user_appearance-default\">\n",
        "      <a class=\"tm-user-info__username\" href=\"/ru/users/darinka666/\">\n",
        "       darinka666\n",
        "       <!-- -->\n",
        "      </a>\n",
        "      <span class=\"tm-article-datetime-published\">\n",
        "       <time datetime=\"2023-11-02T09:22:25.000Z\" title=\"2023-11-02, 12:22\">\n",
        "        11 минут назад\n",
        "       </time>\n",
        "      </span>\n",
        "     </span>\n",
        "    </span>\n",
        "   </div>\n",
        "   <!-- -->\n",
        "  </div>\n",
        "  <h2 class=\"tm-title tm-title_h2\">\n",
        "   <a class=\"tm-title__link\" data-article-link=\"true\" data-test-id=\"article-snippet-title-link\" href=\"/ru/companies/mts_ai/articles/771476/\">\n",
        "    <span>\n",
        "     Обзор Llemma: новая математическая open-source модель\n",
        "    </span>\n",
        "   </a>\n",
        "  </h2>\n",
        "  <div class=\"tm-article-snippet__stats\">\n",
        "   <div class=\"tm-article-complexity tm-article-complexity_complexity-medium\">\n",
        "    <span class=\"tm-svg-icon__wrapper tm-article-complexity__icon\">\n",
        "     <svg class=\"tm-svg-img tm-svg-icon\" height=\"24\" width=\"24\">\n",
        "      <title>\n",
        "       Уровень сложности\n",
        "      </title>\n",
        "      <use xlink:href=\"/img/megazord-v28.2fb1b1c1..svg#complexity-medium\">\n",
        "      </use>\n",
        "     </svg>\n",
        "    </span>\n",
        "    <span class=\"tm-article-complexity__label\">\n",
        "     Средний\n",
        "    </span>\n",
        "   </div>\n",
        "```\n",
        "(и так далее; часть вывода обрезана: например, нет закрывающего тега `</article>`)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "username = soup.find('a', {'class': 'tm-user-info__username'})\n",
        "username.get_text()\n",
        "\n",
        "time = soup.find('time')\n",
        "time.get_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tJMhlQ_idtSJ",
        "outputId": "913798e0-2373-4909-c26b-8594dc319dc2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'26 минут назад'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for article in soup.find_all('article'):\n",
        "  username = article.find('a', {'class': 'tm-user-info__username'}).get_text()\n",
        "  time = article.find('time').get_text()\n",
        "  title = article.find('h2', {'class': 'tm-title tm-title_h2'}).get_text()\n",
        "  print(title, username, time, sep='\\n', end='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "oUpHCEH8fJan",
        "outputId": "2c9a23a7-9cbc-4532-b55d-4b6b0ab77a0c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Многопоточность без боли: моя шпаргалка для собесов в Java\n",
            "MishaBucha \n",
            "26 минут назад\n",
            "\n",
            "RAG+Ragas: учим AI-помощника учить без галлюцинаций\n",
            "gridstan \n",
            "1 час назад\n",
            "\n",
            "Архитектура фронтенда. Навеяно болью от использования FSD\n",
            "I0rrik \n",
            "1 час назад\n",
            "\n",
            "Новый Grok с огромным контекстным окном испытывают на OpenRouter\n",
            "runaway_llm \n",
            "1 час назад\n",
            "\n",
            "На смерть Джеймса Уотсона\n",
            "OlegSivchenko \n",
            "1 час назад\n",
            "\n",
            "ПДн в нашем доме или 152-ФЗ в практике ЖКХ\n",
            "Atolstikov \n",
            "2 часа назад\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'get_text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4259042298.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'tm-user-info__username'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'tm-title tm-title_h2'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtti9jzmYBCr"
      },
      "source": [
        "## Задание на семинар 2\n",
        "\n",
        "Скачать главную страницу Яндекс.Погоды и\n",
        "    \n",
        "- распечатать сегодняшнюю температуру и облачность\n",
        "- распечатать время восхода и заката\n",
        "- погоду на завтра"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhTHnk9obW2_"
      },
      "source": [
        "Простая версия: вместо Яндекс.Погоды возьмите [этот](https://simple-weather-website.netlify.app/) сайт и получите температуру, влажность и скорость ветра."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Краулеры"
      ],
      "metadata": {
        "id": "OEOh-DOqslgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Что такое краулеры?\n",
        "\n",
        "Краулеры $-$ это боты / программы, которые \"ползают\" (*crawl*) по страницам сайта и собирают информацию. Все чаще использование таких программ запрещается правилами пользования сайтами, поэтому это формально нехорошо. Но так продолжают делать и это надо уметь. Запрещают по 2 основным причинам: не хотят делиться данными и боятся, что вы уроните сервер (если сайт $-$ маленький, а сервер $-$ не очень, то это довольно легко).\n",
        "\n",
        "Поэтому нужно собирать данные аккуратно, чтобы:\n",
        "1. вас не заблокировали по IP,\n",
        "2. вы не навредили серверу.\n",
        "\n",
        "Вместо отдельных запросов для этого лучше создать сессию, которая позволит хранить информацию между запросами и поддерживать то же соединение и не создавать каждый раз все заново, что влияет на производительность.\n",
        "\n"
      ],
      "metadata": {
        "id": "ObSjmsGLsm5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint # библиотека для красивого вывода"
      ],
      "metadata": {
        "id": "0f9f_KE2sLmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = requests.session()"
      ],
      "metadata": {
        "id": "IqOulQDCsOE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j24aR3dJfcMZ"
      },
      "source": [
        "Попробуем сделать запрос, просто вместо requests.get мы пишет session.get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05KfZ9L6fcMZ"
      },
      "outputs": [],
      "source": [
        "response = session.get('https://eksmo.ru/khudozhestvennaya-literatura/fantastika/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "prnyFiELtf9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c02ndjjEfcMa"
      },
      "source": [
        "Посмотреть на `headers` запроса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRBqNmsRfcMa"
      },
      "outputs": [],
      "source": [
        "pprint(dict(response.headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGqK7qehfcMa"
      },
      "source": [
        "### Стратегии сбора данных\n",
        "\n",
        "\n",
        "По сути краулеры выполняют сбор страниц (их HTML) как мы это делали на прошлом занятии, но делают они это циклами (или циклами циклов). Можно выделить разные стратегии сбора данных:\n",
        "    \n",
        "**По типу навигации**\n",
        "\n",
        "1. Все страницы со ссылками имеют удобные номера (\"https://ficbook.net/fanfiction/no_fandom/originals?p=2\"), обычно просто `p=<число>` или `page=<число>`. В этом случае вам нужно просто подставлять цифры подробнее про параметры передаваемые в ссылке можно посмотреть [здесь](https://en.wikipedia.org/wiki/Query_string).\n",
        "2. Страницы называются как-то не структурированно (например, по названиям блоков). Тут нужно собирать ссылки на эти страницы и потом по ним ходить и собирать конечные странички.\n",
        "3. Все расположено на одной страничке и догружается с использованием [WebSocket](https://en.wikipedia.org/wiki/WebSocket) или других технологий, при адрес в адресной строке никак не изменяется, данные могут догружаться на сайт автоматически по мере скролла страницы.\n",
        "\n",
        "**По скорости обновления**\n",
        "\n",
        "1. Если сайт довольно статичный по контенту (медленно появляются и удаляются материалы), то можно сначал собрать ссылки, а потом по ним ходить.\n",
        "2. Если сайт очень динамичный по контенту (например, объявления на крупном сайте), вам нужно при получении страничкии ссылок сразу их обходить, а потом переходить к следующей, потому что ко времени получения исчерпывающего списка ссылок по сайту многие будут уже удалены или недоступны.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKxYawjJfcMb"
      },
      "source": [
        "## Блокировки и способы их обхода\n",
        "\n",
        "Для того, чтобы предотвратить автоматический сбор информации с некого сайта, применяются различные инструменты, которые определяют роботов и блокируют запросы с адресов, которые были классифицированы как роботы. Чтобы не заблокировали домашний/учебный IP, лучше сразу задуматься об этих мерах и предотвратить возможные проблемы. Кстати, Википедия не блокирует и можно спокойно скачивать без каких-либо проблем.\n",
        "\n",
        "Чтобы их обойти, можно попробовать несколько инструментов:\n",
        "\n",
        "1. изобразить браузер - при запросе отправляется информация о том, из какого приложения пришел запрос (например, Google Chrome), запросы сделанные из браузера больше похожи на человеческие, для этого нужно задать `user-agent` в параметрах (а его выбирать случайно с помощью `fake_useragent`).\n",
        "1. `time.sleep(x)` - задержка между запросами, чтобы слишком большая скорость запросов не показалась подозрительной или ваши запросы не уронили сервер небольшого ресурса (например, региональной газеты).\n",
        "2. `time.sleep(<случайный промежуток времени>)` - это более хитрая версия, когда время задержки - это случайное число из некоторого отрезка (модуль `random`).\n",
        "4. использовать прокси - существуют ресурсы с бесплатными списками открытых прокси, через которые можно пропускать ваш запрос и сервер будет думать, что запросы приходят из разных мест (anonymous и elite классы прокси) или использовать анонимизированные сети к примеру сеть [Tor](https://en.wikipedia.org/wiki/Tor_(network)) и аналоги.\n",
        "\n",
        "При работе в Google Colab зачастую помогает перезапускать среду: код в Colab выполняется на внешних серверах, соответственно, при перезапуске будет отправлять запросы уже с другого сервера. Хотя иногда это приводит и к обратным проблемам: не все сайты доступны вне России, например (как и не все доступны из России))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_7ftEm3fcMb"
      },
      "source": [
        "### Притвориться нормальным браузером"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fake_useragent"
      ],
      "metadata": {
        "id": "6DER8Th3h_W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSN-IbsIfcMb"
      },
      "outputs": [],
      "source": [
        "from fake_useragent import UserAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onloj0LjfcMb"
      },
      "source": [
        "Можно настроить так, чтобы не проверять безопасность соединения, что иногда вызывает ошибки. Но это можно делать с сайтами, которым вы доверяете."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_paYypB_fcMb"
      },
      "outputs": [],
      "source": [
        "ua = UserAgent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqVD806rfcMc"
      },
      "outputs": [],
      "source": [
        "headers = {'User-Agent': ua.random}\n",
        "# другой способ\n",
        "# headers = {'User-Agent': ua.chrome}\n",
        "print(headers)\n",
        "response = session.get('https://eksmo.ru/khudozhestvennaya-literatura/fantastika/', headers=headers)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "DRanyGbwjb76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlGbQqbSfcMb"
      },
      "source": [
        "### Пауза между запросами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRU6AGoDfcMb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yukk1ZZZfcMb"
      },
      "outputs": [],
      "source": [
        "for _ in range(5):\n",
        "    response = session.get('https://eksmo.ru/khudozhestvennaya-literatura/fantastika/')\n",
        "    print(datetime.now(), response)\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7r2N1JxfcMc"
      },
      "source": [
        "Некоторые сайты замечают подозрительно регулярные запросы. `random.uniform` позволяет получить случайное число из отрезка и сделать наши запросы менее подозрительными."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVOizC3BfcMc"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5CqYoyWfcMc"
      },
      "outputs": [],
      "source": [
        "random.uniform(1, 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = session.get('https://eksmo.ru/khudozhestvennaya-literatura/fantastika/')"
      ],
      "metadata": {
        "id": "iVXJ5BE9xqze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjJSWX5dfcMc"
      },
      "outputs": [],
      "source": [
        "for _ in range(5):\n",
        "    response = session.get('https://eksmo.ru/khudozhestvennaya-literatura/fantastika/')\n",
        "    print(datetime.now(), response)\n",
        "    time.sleep(random.uniform(1.1, 5.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "А всё, поздно уже.. (Статус 403 - Forbidden: сервер получил наш запрос, но отказался на него отвечать)\n",
        "\n",
        "В идеале все перечисленные способы нужно использовать сразу, особенно если вы работаете локально, иначе сайт мжет вас запомнить и не пускать, например, несколько часов."
      ],
      "metadata": {
        "id": "b3auLARuxvqW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGao7LmPfcMc"
      },
      "source": [
        "### Подключение через прокси\n",
        "\n",
        "Прокси-сервер — это дополнительное звено между вами и интернетом, через него пойдет подключение и сайт не будет знать, что это вы посылаете запрос.\n",
        "\n",
        "1. Адреса прокси можно взять со специальных сайтов: например, [https://hideip.me/ru/proxy/httplist](https://hideip.me/ru/proxy/httplist).\n",
        "2. Потом проверить, что они рабочие, прежде чем использовать: [https://checkerproxy.net/](https://checkerproxy.net/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40HlZ1RzfcMc"
      },
      "outputs": [],
      "source": [
        "proxy = {\"https://\": \"https://176.213.141.107:8080\"}\n",
        "\n",
        "response = session.get('https://eksmo.ru/khudozhestvennaya-literatura/fantastika/', proxies=proxy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "25evwy4sklhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL_DJuFFfcMd"
      },
      "source": [
        "## Пример\n",
        "\n",
        "Давайте обкачаем немного новостей с сайта вышки.\n",
        "\n",
        "1. Страницы имеют вид \"https://www.hse.ru/news/page1.html\", поэтому можно просто идти циклом.\n",
        "2. Достанем дату публикации, заголовок, краткое описание (из станицы со списком новостей), текст полной статьи и метки (из самой страницы новости)\n",
        "3. Сохраним в датафрейм."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77lsQnUvfcMd"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REqAv5QifcMd"
      },
      "source": [
        "### **Шаг 1. Найти страницы**\n",
        "\n",
        "Посмотрим, как устроены новости и скачаем одну страницу:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyP7_suAfcMd"
      },
      "outputs": [],
      "source": [
        "page_number = 3\n",
        "url = f'https://www.hse.ru/news/page{page_number}.html'\n",
        "req = session.get(url, headers={'User-Agent': ua.random})\n",
        "page = req.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjX52-u8fcMd"
      },
      "source": [
        "Распарсим с помощью `BeautifulSoup`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPGo11t3fcMd"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(page, 'html.parser')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBvTaPhCfcMd"
      },
      "source": [
        "Найдем отдельные посты:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZwTr0ycfcMd"
      },
      "outputs": [],
      "source": [
        "news = soup.find_all('div', {'class': 'post'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S93dR2ncfcMe"
      },
      "outputs": [],
      "source": [
        "len(news)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(news[0].prettify())"
      ],
      "metadata": {
        "id": "PeM7OOmeljBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssFrQPFVfcMe"
      },
      "source": [
        "Найдем заголовок-ссылку и запомним текст заголовка:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e77UhAdnfcMe"
      },
      "outputs": [],
      "source": [
        "title_obj = news[0].find('a')\n",
        "title_obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYXfAR5YfcMf"
      },
      "outputs": [],
      "source": [
        "title = title_obj.text\n",
        "title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyn7txeHfcMf"
      },
      "source": [
        "Достанем свойства этой ссылки (куда ведет, `class`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0tanBomfcMf"
      },
      "outputs": [],
      "source": [
        "attrs = title_obj.attrs\n",
        "attrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_opKEwzfcMf"
      },
      "source": [
        "Достанем саму ссылку:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vkt1lshfcMf"
      },
      "outputs": [],
      "source": [
        "href = title_obj.attrs['href']\n",
        "href"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCw4UeANfcMf"
      },
      "source": [
        "Достанем текст новости:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qptpLqqsfcMf"
      },
      "outputs": [],
      "source": [
        "short_text = news[0].find('div', {'class': 'post__text'}).text\n",
        "short_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEipaohCfcMg"
      },
      "source": [
        "Достанем день, месяц, год публикации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdEViI7tfcMg"
      },
      "outputs": [],
      "source": [
        "pub_day = news[0].find('div', {'class': 'post-meta__day'}).text\n",
        "pub_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKPlCTJSfcMg"
      },
      "outputs": [],
      "source": [
        "pub_month = news[0].find('div', {'class': 'post-meta__month'}).text\n",
        "pub_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7ZyLQLYfcMg"
      },
      "outputs": [],
      "source": [
        "pub_year = news[0].find('div', {'class': 'post-meta__year'}).text\n",
        "pub_year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmr5-qhCfcMg"
      },
      "source": [
        "### **Шаг 2. Научиться парсить страничку самой новости**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhRuE2qifcMg"
      },
      "source": [
        "Скачаем и распарсим полученную ссылку:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bURGbQxfcMh"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.hse.ru/news/science/1099260087.html'\n",
        "\n",
        "req = session.get(url, headers={'User-Agent': ua.random})\n",
        "page = req.text\n",
        "\n",
        "soup = BeautifulSoup(page, 'html.parser')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLCCEjnJfcMh"
      },
      "source": [
        "Сохраним текст, распечатаем кусочек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KAJX169fcMh"
      },
      "outputs": [],
      "source": [
        "full_text = soup.find('div', {'class': 'post__content'}).text\n",
        "full_text[:200]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHr-LveJfcMh"
      },
      "source": [
        "Найдем теги, которые присвоены статье:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XPdl4mLfcMh"
      },
      "outputs": [],
      "source": [
        "meta = soup.find('div', {'class': 'articleMeta'})\n",
        "\n",
        "tags = meta.find_all('a', {'class': 'tag'})\n",
        "tags = [t.text for t in tags]\n",
        "tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i7ewcgAfcMh"
      },
      "source": [
        "### **Шаг 3. Оформляем нормально в функции**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FinshHq9fcMh"
      },
      "source": [
        "Сделаем словарь соответствий имени месяца и его номера:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqB_8ctbfcMh"
      },
      "outputs": [],
      "source": [
        "months = {\n",
        "    value: key + 1\n",
        "    for key, value in enumerate(\n",
        "        ['янв', 'фев', 'мар', 'апр', 'мая', 'июн', 'июл', 'авг', 'сен', 'окт', 'ноя', 'дек']\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmNoWpaGfcMh"
      },
      "source": [
        "Парсим информацию из страницы со списком новостей (блок одной новости):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN9BzbhYfcMh"
      },
      "outputs": [],
      "source": [
        "def parse_news_page_block(one_block):\n",
        "    block = {}\n",
        "    a = one_block.find('a')\n",
        "    block['title'] = a.text\n",
        "    block['href'] = a.attrs['href']\n",
        "    block['short_text'] = one_block.find('div', {'class': 'post__text'}).text\n",
        "    block['pub_day'] = int(one_block.find('div', {'class': 'post-meta__day'}).text)\n",
        "    block['pub_month'] = months[one_block.find('div', {'class': 'post-meta__month'}).text]\n",
        "    block['pub_year'] = int(one_block.find('div', {'class': 'post-meta__year'}).text)\n",
        "    return block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY4MVAGSfcMi"
      },
      "source": [
        "Парсим отдельную страницу новости:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ4esh8KfcMi"
      },
      "outputs": [],
      "source": [
        "def parse_one_article(block):\n",
        "    url_one = 'http://www.hse.ru' + block['href']\n",
        "    req = session.get(url_one, headers={'User-Agent': ua.random})\n",
        "    page = req.text\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "    block['full_text'] = soup.find('div', {'class': 'post__content'}).text\n",
        "    meta = soup.find('div', {'class': 'articleMeta'})\n",
        "    tags = meta.find_all('a', {'class': 'tag'})\n",
        "    block['tags'] = [t.text for t in tags]\n",
        "    return block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_FSEM1TfcMi"
      },
      "source": [
        "Регулярное выражение для того, чтобы достать ID новости и не повторяться:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekRFRt4RfcMi"
      },
      "outputs": [],
      "source": [
        "regex_hse_id = re.compile('/([0-9]*?).html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzB596T5fcMi"
      },
      "source": [
        "Обработать N-ую страницу новостей:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioB1E3odfcMi"
      },
      "outputs": [],
      "source": [
        "def get_nth_page(page_number):\n",
        "    # скачиваем\n",
        "    url = f'https://www.hse.ru/news/page{page_number}.html'\n",
        "    req = session.get(url, headers={'User-Agent': ua.random})\n",
        "    page = req.text\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "\n",
        "    # находим новости\n",
        "    news = soup.find_all('div', {'class': 'post'})\n",
        "\n",
        "    # идем по новостям и обрабатываем их\n",
        "    blocks = []\n",
        "    for n in news:\n",
        "        try:\n",
        "            blocks.append(parse_news_page_block(n))\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    # идем по отдельным статьям и достаем информацию\n",
        "    result = []\n",
        "    for b in blocks:\n",
        "        if b['href'].startswith('/'):\n",
        "            idx = regex_hse_id.findall(b['href'])[0]\n",
        "            try:\n",
        "                res = parse_one_article(b)\n",
        "                res['hse_id'] = idx\n",
        "                result.append(res)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "\n",
        "    # возвращаем найденную информацию\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOYX5UtyfcMi"
      },
      "source": [
        "### **Шаг 4. Сохраняем в датафрейм**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tba-meZMfcMi"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_cxdEkNfcMi"
      },
      "source": [
        "Напишем функцию, куда передаем количество страниц и она выполняет все нужные действия:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1N6DjmYfcMi"
      },
      "outputs": [],
      "source": [
        "def run_all(n_pages):\n",
        "    blocks = []\n",
        "    for i in tqdm(range(n_pages)):\n",
        "        blocks.extend(get_nth_page(i+1))\n",
        "\n",
        "    return blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U_8A7XefcMj"
      },
      "source": [
        "Запускаем на 20 первых страниц:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slgGkQ1efcMj"
      },
      "outputs": [],
      "source": [
        "blocks = run_all(3)\n",
        "\n",
        "df = pd.DataFrame(blocks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(3)"
      ],
      "metadata": {
        "id": "TFwuTJs639qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QizlVBKBfcMj"
      },
      "source": [
        "Посмотрим на 10 самых популярных тегов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2dURsyvfcMj"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCpGXDrOfcMj"
      },
      "outputs": [],
      "source": [
        "tags = [tag for tags in df[\"tags\"] for tag in tags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjaywAcMfcMj"
      },
      "outputs": [],
      "source": [
        "for title, counts in sorted(Counter(tags).items(), key=lambda x: -x[-1])[:10]:\n",
        "    print(counts,\"\\t\", title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pDJryZVfcMj"
      },
      "source": [
        "Посмотрим, сколько публикаций по месяцам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t-X0MGFfcMj"
      },
      "outputs": [],
      "source": [
        "df.groupby(\"pub_month\")[\"hse_id\"].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание на семинар\n",
        "\n",
        "Сделайте краулер фильмов аналогичный тому, что выше.\n",
        "\n",
        "У вас есть список на [Letterboxd](https://letterboxd.com/) c перечислением всех фильмов из книжки *1001 Movies You Must See Before You Die*: https://letterboxd.com/peterstanley/list/1001-movies-you-must-see-before-you-die/detail/.\n",
        "\n",
        "1. Все страницы выглядят как `https://letterboxd.com/peterstanley/list/1001-movies-you-must-see-before-you-die/detail/page/<номер>/`, поэтому снова можно идти циклом.\n",
        "2. Достанем номер фильма, его название, год выпуска со страницы списка.\n",
        "3. Достанем режиссера, слоган (если есть) и краткое описание с самой страницы фильма.\n",
        "4. Сохраним в датафрейм."
      ],
      "metadata": {
        "id": "q62IwDVvltOv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLNvvYSN9T3O"
      },
      "source": [
        "## *Решение задания на семинар 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3qu0TGk9T3O"
      },
      "outputs": [],
      "source": [
        "for post in soup.find_all(\"article\"):\n",
        "    try:  # чтобы исключить случаи, когда нет юзернейма / времени / заголовка\n",
        "        user_name = post.find('a', {'class': 'tm-user-info__username'}).get_text()\n",
        "        time_article = post.find('time').get_text()\n",
        "        header_article = post.find('h2', {'class': 'tm-title tm-title_h2'}).get_text()\n",
        "        print('Name:', user_name)\n",
        "        print('Time:', time_article)\n",
        "        print('Header:', header_article, end='\\n\\n')\n",
        "    except AttributeError:\n",
        "        # ошибка AttributeError возникает, когда от объекта типа None вызывают метод .get_text()\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}